{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f372824-4964-4ed5-bf7b-7669fc3c8d75",
   "metadata": {},
   "source": [
    "# IBM watsonx.ai - Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7c43d-f921-441d-8ee8-9f1386739b2b",
   "metadata": {},
   "source": [
    "## Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8070068b-8eb6-4fa5-ae51-055263eba00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IBM_WATSONXAI_ENDPOINT\"] = \"<value>\"\n",
    "os.environ[\"IBM_API_KEY\"] = \"<value>\"\n",
    "os.environ[\"IBM_PROJECT_ID\"] = \"<value>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf76c1-46c2-4c78-9c37-c99b86c9a7ba",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1478ee39-2c4e-4f36-a05c-954826793a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc257b-03bf-4e4d-ae43-7afaec61d101",
   "metadata": {},
   "source": [
    "## Define Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4620bf55-f07a-4524-bb8b-e28f4ca8dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "\n",
    "# method - get_watsonxai_response\n",
    "def get_watsonxai_response (\n",
    "    modelId, input_text, custom_instruction, max_new_tokens, temperature, top_p\n",
    "):\n",
    "\n",
    "    # model params\n",
    "    generate_params = {\n",
    "        GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,\n",
    "        GenParams.MIN_NEW_TOKENS: 0,\n",
    "        GenParams.MAX_NEW_TOKENS: max_new_tokens,\n",
    "        GenParams.REPETITION_PENALTY: 1,\n",
    "        GenParams.TEMPERATURE: temperature,\n",
    "        GenParams.TOP_P: top_p,\n",
    "        GenParams.TOP_K: 50\n",
    "    }\n",
    "\n",
    "    # model choices\n",
    "    model_id_str = \"\"\n",
    "    if modelId == \"ibm/granite-13b-chat-v1\":\n",
    "        model_id_str = ModelTypes.GRANITE_13B_CHAT\n",
    "    elif modelId == \"meta-llama/llama-2-70b-chat\":\n",
    "        model_id_str = ModelTypes.LLAMA_2_70B_CHAT\n",
    "\n",
    "    # capture environment variables\n",
    "    env_ibm_api_key = os.environ.get(\"IBM_API_KEY\")\n",
    "    env_ibm_watsonxai_endpoint = os.environ.get(\"IBM_WATSONXAI_ENDPOINT\")\n",
    "    env_ibm_project_id = os.environ.get(\"IBM_PROJECT_ID\")\n",
    "\n",
    "    # prepare model input\n",
    "    model = Model(\n",
    "        model_id = model_id_str,\n",
    "        params = generate_params,\n",
    "        credentials = {\n",
    "            \"apikey\": env_ibm_api_key,\n",
    "            \"url\": env_ibm_watsonxai_endpoint\n",
    "        },\n",
    "        project_id = env_ibm_project_id\n",
    "    )\n",
    "\n",
    "    # model invocation\n",
    "    input_prompt = custom_instruction + \"\\nInput: \\n\" + input_text + \"Output:\"\n",
    "    generated_response = model.generate_text(prompt=input_prompt)\n",
    "    return generated_response\n",
    "# end of get_watsonxai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efeeb6-608d-475a-8c2e-5cdd81e4a853",
   "metadata": {},
   "source": [
    "## Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e38760-7801-4a92-9293-0cf69d2c972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = \"ibm/granite-13b-chat-v1\" # options: ibm/granite-13b-chat-v1, meta-llama/llama-2-70b-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962bc69-f9a1-45b6-b7e2-bbd8a83ea716",
   "metadata": {},
   "source": [
    "## Provide custom instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaf52633-9539-4586-ae13-53698c6bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_instruction = \"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aad0a2-225f-4ec9-b234-8074fa6c9032",
   "metadata": {},
   "source": [
    "## Provide input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1d364bd-dc2b-4f3f-8244-c478fda2d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = '''A large language model (LLM) is a type of language model notable for its ability to achieve general-purpose language understanding and generation. LLMs acquire these abilities by using massive amounts of data to learn billions of parameters during training and consuming large computational resources during their training and operation. LLMs are artificial neural networks (mainly transformers) and are (pre-)trained using self-supervised learning and semi-supervised learning. \n",
    "As autoregressive language models, they work by taking an input text and repeatedly predicting the next token or word. Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire embodied knowledge about syntax, semantics and ontology inherent in human language corpora, but also inaccuracies and biases present in the corpora. \n",
    "Notable examples include OpenAI's GPT models (e.g., GPT-3.5 and GPT-4, used in ChatGPT), Google's PaLM (used in Bard), and Meta's LLaMa, as well as BLOOM, Ernie 3.0 Titan, and Anthropic's Claude 2.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90497c46-a390-4e85-9a09-bb44307eff75",
   "metadata": {},
   "source": [
    "## Inference API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2f3a731-4399-4578-8f66-c9f97f4e94cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LLMs are language models that are trained on large amounts of text to achieve general-purpose understanding and generation. They are a class of artificial neural networks (mainly transformers) that are trained using self-supervised learning and semi-supervised learning.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_watsonxai_response(model_choice, input_text, custom_instruction, 512, 0.7, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
